{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalación de librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\leopa\\anaconda3\\lib\\site-packages (3.7)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\leopa\\anaconda3\\lib\\site-packages (from nltk) (2022.3.15)\n",
      "Requirement already satisfied: click in c:\\users\\leopa\\anaconda3\\lib\\site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: tqdm in c:\\users\\leopa\\anaconda3\\lib\\site-packages (from nltk) (4.64.0)\n",
      "Requirement already satisfied: joblib in c:\\users\\leopa\\anaconda3\\lib\\site-packages (from nltk) (1.1.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\leopa\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.4)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk\n",
    "pip install svgling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet2021 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet31 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk; nltk.download('popular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tokenización - Métodos para dividir texto en palabras o frases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['At',\n",
       " 'eight',\n",
       " \"o'clock\",\n",
       " 'on',\n",
       " 'Thursday',\n",
       " 'morning',\n",
       " 'Arthur',\n",
       " 'did',\n",
       " \"n't\",\n",
       " 'feel',\n",
       " 'very',\n",
       " 'good',\n",
       " '.']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence = \"\"\"At eight o'clock on Thursday morning Arthur didn't feel very good.\"\"\"\n",
    "tokens = nltk.word_tokenize(sentence)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "\n",
    "texto = \"NLTK es una plataforma líder para construir programas en Python que trabajen con datos en lenguaje humano.\"\n",
    "palabras = word_tokenize(texto)\n",
    "frases = sent_tokenize(texto)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK es una plataforma líder para construir programas en Python que trabajen con datos en lenguaje humano.']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'es',\n",
       " 'una',\n",
       " 'plataforma',\n",
       " 'líder',\n",
       " 'para',\n",
       " 'construir',\n",
       " 'programas',\n",
       " 'en',\n",
       " 'Python',\n",
       " 'que',\n",
       " 'trabajen',\n",
       " 'con',\n",
       " 'datos',\n",
       " 'en',\n",
       " 'lenguaje',\n",
       " 'humano',\n",
       " '.']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.- Eliminación de palabras vacias - NLTK puede eliminar palabras vacias en varios idiomas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "palabras = word_tokenize(texto)\n",
    "palabras_filtradas = [palabra for palabra in palabras if palabra.lower() not in stopwords.words('spanish')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NLTK',\n",
       " 'plataforma',\n",
       " 'líder',\n",
       " 'construir',\n",
       " 'programas',\n",
       " 'Python',\n",
       " 'trabajen',\n",
       " 'datos',\n",
       " 'lenguaje',\n",
       " 'humano',\n",
       " '.']"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras_filtradas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.- Etiquetas - NLTK puede asignar etiquetado de partes de la oración:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('At', 'IN'),\n",
       " ('eight', 'CD'),\n",
       " (\"o'clock\", 'NN'),\n",
       " ('on', 'IN'),\n",
       " ('Thursday', 'NNP'),\n",
       " ('morning', 'NN')]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged = nltk.pos_tag(tokens)\n",
    "tagged[0:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "\n",
    "palabras = word_tokenize(texto)\n",
    "etiquetas_pos = pos_tag(palabras)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NLTK', 'NNP'),\n",
       " ('es', 'CC'),\n",
       " ('una', 'JJ'),\n",
       " ('plataforma', 'NN'),\n",
       " ('líder', 'NN'),\n",
       " ('para', 'NN'),\n",
       " ('construir', 'NN'),\n",
       " ('programas', 'NN'),\n",
       " ('en', 'IN'),\n",
       " ('Python', 'NNP'),\n",
       " ('que', 'NN'),\n",
       " ('trabajen', 'NN'),\n",
       " ('con', 'NN'),\n",
       " ('datos', 'NN'),\n",
       " ('en', 'FW'),\n",
       " ('lenguaje', 'JJ'),\n",
       " ('humano', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.- Reconocimiento de entidades nombradas - NLTK es compantible con el reconocimiento de entidades nombradas: nombres, Ubicaciones y fechas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "texto = \"Barack Obama nació en Hawái.\"\n",
    "palabras = word_tokenize(texto)\n",
    "etiquetas_pos = pos_tag(palabras)\n",
    "etiquetas_ner = ne_chunk(etiquetas_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Barack', 'Obama', 'nació', 'en', 'Hawái', '.']"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Barack', 'NNP'),\n",
       " ('Obama', 'NNP'),\n",
       " ('nació', 'MD'),\n",
       " ('en', 'VB'),\n",
       " ('Hawái', 'NNP'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px;\" version=\"1.1\" viewBox=\"0,0,304.0,168.0\" width=\"304px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"21.0526%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Barack</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"10.5263%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"21.0526%\" x=\"21.0526%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Obama</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.5789%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"18.4211%\" x=\"42.1053%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">nació</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">MD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.3158%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"10.5263%\" x=\"60.5263%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">en</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VB</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65.7895%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"21.0526%\" x=\"71.0526%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Hawái</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"81.5789%\" y1=\"19.2px\" y2=\"48px\" /><svg width=\"7.89474%\" x=\"92.1053%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"19.2px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"96.0526%\" y1=\"19.2px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Barack', 'NNP')]), Tree('PERSON', [('Obama', 'NNP')]), ('nació', 'MD'), ('en', 'VB'), Tree('PERSON', [('Hawái', 'NNP')]), ('.', '.')])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "etiquetas_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5.- Reducción de palabras - NLTK proporciona herramientas para reducir las palabras a sus formas mediante procesos de reduccion y lematización:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = SnowballStemmer(\"spanish\")\n",
    "lematizador = WordNetLemmatizer()\n",
    "\n",
    "palabra = \"corriendo\"\n",
    "palabra_reducida = stemmer.stem(palabra)\n",
    "palabra_lematizada = lematizador.lemmatize(palabra)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corriendo'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corr'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabra_reducida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corriendo'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "palabra_lematizada"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análisis de Sentimiento con NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package subjectivity to\n",
      "[nltk_data]     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package subjectivity is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('subjectivity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.corpus import subjectivity\n",
    "from nltk.sentiment import SentimentAnalyzer\n",
    "from nltk.sentiment.util import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 100)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_instances = 100\n",
    "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_instances]]\n",
    "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_instances]]\n",
    "len(subj_docs), len(obj_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['smart',\n",
       "  'and',\n",
       "  'alert',\n",
       "  ',',\n",
       "  'thirteen',\n",
       "  'conversations',\n",
       "  'about',\n",
       "  'one',\n",
       "  'thing',\n",
       "  'is',\n",
       "  'a',\n",
       "  'small',\n",
       "  'gem',\n",
       "  '.'],\n",
       " 'subj')"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subj_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['the',\n",
       "  'movie',\n",
       "  'begins',\n",
       "  'in',\n",
       "  'the',\n",
       "  'past',\n",
       "  'where',\n",
       "  'a',\n",
       "  'young',\n",
       "  'boy',\n",
       "  'named',\n",
       "  'sam',\n",
       "  'attempts',\n",
       "  'to',\n",
       "  'save',\n",
       "  'celebi',\n",
       "  'from',\n",
       "  'a',\n",
       "  'hunter',\n",
       "  '.'],\n",
       " 'obj')"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obj_docs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Separación de training y testing para hacer la clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subj_docs = subj_docs[:80]\n",
    "test_subj_docs = subj_docs[80:100]\n",
    "train_obj_docs = obj_docs[:80]\n",
    "test_obj_docs = obj_docs[80:100]\n",
    "training_docs = train_subj_docs+train_obj_docs\n",
    "testing_docs = test_subj_docs+test_obj_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aplicación de análisis de sentimiento:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer = SentimentAnalyzer()\n",
    "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unigram_feats = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=4)\n",
    "len(unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_set = sentim_analyzer.apply_features(training_docs)\n",
    "test_set = sentim_analyzer.apply_features(testing_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training classifier\n"
     ]
    }
   ],
   "source": [
    "trainer = NaiveBayesClassifier.train\n",
    "classifier = sentim_analyzer.train(trainer, training_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Resultados del clasificador de Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating NaiveBayesClassifier results...\n",
      "Accuracy: 0.8\n",
      "F-measure [obj]: 0.8\n",
      "F-measure [subj]: 0.8\n",
      "Precision [obj]: 0.8\n",
      "Precision [subj]: 0.8\n",
      "Recall [obj]: 0.8\n",
      "Recall [subj]: 0.8\n"
     ]
    }
   ],
   "source": [
    "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
    "    print('{0}: {1}'.format(key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nueva aplicación de análisis de sentimiento usando Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sentences = [\"VADER is smart, handsome, and funny.\", # positive sentence example\n",
    "   \"VADER is smart, handsome, and funny!\", # punctuation emphasis handled correctly (sentiment intensity adjusted)\n",
    "   \"VADER is very smart, handsome, and funny.\",  # booster words handled correctly (sentiment intensity adjusted)\n",
    "   \"VADER is VERY SMART, handsome, and FUNNY.\",  # emphasis for ALLCAPS handled\n",
    "   \"VADER is VERY SMART, handsome, and FUNNY!!!\",# combination of signals - VADER appropriately adjusts intensity\n",
    "   \"VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\",# booster words & punctuation make this close to ceiling for score\n",
    "   \"The book was good.\",         # positive sentence\n",
    "   \"The book was kind of good.\", # qualified positive sentence is handled correctly (intensity adjusted)\n",
    "   \"The plot was good, but the characters are uncompelling and the dialog is not great.\", # mixed negation sentence\n",
    "   \"A really bad, horrible book.\",       # negative sentence with booster words\n",
    "   \"At least it isn't a horrible book.\", # negated negative sentence with contraction\n",
    "   \":) and :D\",     # emoticons handled\n",
    "   \"\",              # an empty string is correctly handled\n",
    "   \"Today sux\",     #  negative slang handled\n",
    "   \"Today sux!\",    #  negative slang with punctuation emphasis handled\n",
    "   \"Today SUX!\",    #  negative slang with capitalization emphasis\n",
    "   \"Today kinda sux! But I'll get by, lol\" # mixed sentiment example with slang and constrastive conjunction \"but\"\n",
    " ]\n",
    "paragraph = \"It was one of the worst movies I've seen, despite good reviews. \\\n",
    " Unbelievably bad acting!! Poor direction. VERY poor production. \\\n",
    " The movie was bad. Very bad movie. VERY bad movie. VERY BAD movie. VERY BAD movie!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import tokenize\n",
    "lines_list = tokenize.sent_tokenize(paragraph)\n",
    "sentences.extend(lines_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "tricky_sentences = [\n",
    "   \"Most automated sentiment analysis tools are shit.\",\n",
    "   \"VADER sentiment analysis is the shit.\",\n",
    "   \"Sentiment analysis has never been good.\",\n",
    "   \"Sentiment analysis with VADER has never been this good.\",\n",
    "   \"Warren Beatty has never been so entertaining.\",\n",
    "   \"I won't say that the movie is astounding and I wouldn't claim that \\\n",
    "   the movie is too banal either.\",\n",
    "   \"I like to hate Michael Bay films, but I couldn't fault this one\",\n",
    "   \"I like to hate Michael Bay films, BUT I couldn't help but fault this one\",\n",
    "   \"It's one thing to watch an Uwe Boll film, but another thing entirely \\\n",
    "   to pay for it\",\n",
    "   \"The movie was too good\",\n",
    "   \"This movie was actually neither that funny, nor super witty.\",\n",
    "   \"This movie doesn't care about cleverness, wit or any other kind of \\\n",
    "   intelligent humor.\",\n",
    "   \"Those who find ugly meanings in beautiful things are corrupt without \\\n",
    "   being charming.\",\n",
    "   \"There are slow and repetitive parts, BUT it has just enough spice to \\\n",
    "   keep it interesting.\",\n",
    "   \"The script is not fantastic, but the acting is decent and the cinematography \\\n",
    "   is EXCELLENT!\",\n",
    "   \"Roger Dodger is one of the most compelling variations on this theme.\",\n",
    "   \"Roger Dodger is one of the least compelling variations on this theme.\",\n",
    "   \"Roger Dodger is at least compelling as a variation on the theme.\",\n",
    "   \"they fall in love with the product\",\n",
    "   \"but then it breaks\",\n",
    "   \"usually around the time the 90 day warranty expires\",\n",
    "   \"the twin towers collapsed today\",\n",
    "   \"However, Mr. Carter solemnly argues, his client carried out the kidnapping \\\n",
    "   under orders and in the ''least offensive way possible.''\"\n",
    " ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VADER is smart, handsome, and funny.\n",
      "compound: 0.8316, neg: 0.0, neu: 0.254, pos: 0.746, \n",
      "VADER is smart, handsome, and funny!\n",
      "compound: 0.8439, neg: 0.0, neu: 0.248, pos: 0.752, \n",
      "VADER is very smart, handsome, and funny.\n",
      "compound: 0.8545, neg: 0.0, neu: 0.299, pos: 0.701, \n",
      "VADER is VERY SMART, handsome, and FUNNY.\n",
      "compound: 0.9227, neg: 0.0, neu: 0.246, pos: 0.754, \n",
      "VADER is VERY SMART, handsome, and FUNNY!!!\n",
      "compound: 0.9342, neg: 0.0, neu: 0.233, pos: 0.767, \n",
      "VADER is VERY SMART, really handsome, and INCREDIBLY FUNNY!!!\n",
      "compound: 0.9469, neg: 0.0, neu: 0.294, pos: 0.706, \n",
      "The book was good.\n",
      "compound: 0.4404, neg: 0.0, neu: 0.508, pos: 0.492, \n",
      "The book was kind of good.\n",
      "compound: 0.3832, neg: 0.0, neu: 0.657, pos: 0.343, \n",
      "The plot was good, but the characters are uncompelling and the dialog is not great.\n",
      "compound: -0.7042, neg: 0.327, neu: 0.579, pos: 0.094, \n",
      "A really bad, horrible book.\n",
      "compound: -0.8211, neg: 0.791, neu: 0.209, pos: 0.0, \n",
      "At least it isn't a horrible book.\n",
      "compound: 0.431, neg: 0.0, neu: 0.637, pos: 0.363, \n",
      ":) and :D\n",
      "compound: 0.7925, neg: 0.0, neu: 0.124, pos: 0.876, \n",
      "\n",
      "compound: 0.0, neg: 0.0, neu: 0.0, pos: 0.0, \n",
      "Today sux\n",
      "compound: -0.3612, neg: 0.714, neu: 0.286, pos: 0.0, \n",
      "Today sux!\n",
      "compound: -0.4199, neg: 0.736, neu: 0.264, pos: 0.0, \n",
      "Today SUX!\n",
      "compound: -0.5461, neg: 0.779, neu: 0.221, pos: 0.0, \n",
      "Today kinda sux! But I'll get by, lol\n",
      "compound: 0.5249, neg: 0.138, neu: 0.517, pos: 0.344, \n",
      "It was one of the worst movies I've seen, despite good reviews.\n",
      "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
      "Unbelievably bad acting!!\n",
      "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
      "Poor direction.\n",
      "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
      "VERY poor production.\n",
      "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
      "The movie was bad.\n",
      "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
      "Very bad movie.\n",
      "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
      "VERY bad movie.\n",
      "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
      "VERY BAD movie.\n",
      "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
      "VERY BAD movie!\n",
      "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n",
      "It was one of the worst movies I've seen, despite good reviews.\n",
      "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
      "Unbelievably bad acting!!\n",
      "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
      "Poor direction.\n",
      "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
      "VERY poor production.\n",
      "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
      "The movie was bad.\n",
      "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
      "Very bad movie.\n",
      "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
      "VERY bad movie.\n",
      "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
      "VERY BAD movie.\n",
      "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
      "VERY BAD movie!\n",
      "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n",
      "It was one of the worst movies I've seen, despite good reviews.\n",
      "compound: -0.7584, neg: 0.394, neu: 0.606, pos: 0.0, \n",
      "Unbelievably bad acting!!\n",
      "compound: -0.6572, neg: 0.686, neu: 0.314, pos: 0.0, \n",
      "Poor direction.\n",
      "compound: -0.4767, neg: 0.756, neu: 0.244, pos: 0.0, \n",
      "VERY poor production.\n",
      "compound: -0.6281, neg: 0.674, neu: 0.326, pos: 0.0, \n",
      "The movie was bad.\n",
      "compound: -0.5423, neg: 0.538, neu: 0.462, pos: 0.0, \n",
      "Very bad movie.\n",
      "compound: -0.5849, neg: 0.655, neu: 0.345, pos: 0.0, \n",
      "VERY bad movie.\n",
      "compound: -0.6732, neg: 0.694, neu: 0.306, pos: 0.0, \n",
      "VERY BAD movie.\n",
      "compound: -0.7398, neg: 0.724, neu: 0.276, pos: 0.0, \n",
      "VERY BAD movie!\n",
      "compound: -0.7616, neg: 0.735, neu: 0.265, pos: 0.0, \n",
      "Most automated sentiment analysis tools are shit.\n",
      "compound: -0.5574, neg: 0.375, neu: 0.625, pos: 0.0, \n",
      "VADER sentiment analysis is the shit.\n",
      "compound: 0.6124, neg: 0.0, neu: 0.556, pos: 0.444, \n",
      "Sentiment analysis has never been good.\n",
      "compound: -0.3412, neg: 0.325, neu: 0.675, pos: 0.0, \n",
      "Sentiment analysis with VADER has never been this good.\n",
      "compound: 0.5228, neg: 0.0, neu: 0.703, pos: 0.297, \n",
      "Warren Beatty has never been so entertaining.\n",
      "compound: 0.5777, neg: 0.0, neu: 0.616, pos: 0.384, \n",
      "I won't say that the movie is astounding and I wouldn't claim that    the movie is too banal either.\n",
      "compound: 0.4215, neg: 0.0, neu: 0.851, pos: 0.149, \n",
      "I like to hate Michael Bay films, but I couldn't fault this one\n",
      "compound: 0.3153, neg: 0.157, neu: 0.534, pos: 0.309, \n",
      "I like to hate Michael Bay films, BUT I couldn't help but fault this one\n",
      "compound: -0.1531, neg: 0.277, neu: 0.477, pos: 0.246, \n",
      "It's one thing to watch an Uwe Boll film, but another thing entirely    to pay for it\n",
      "compound: -0.2541, neg: 0.112, neu: 0.888, pos: 0.0, \n",
      "The movie was too good\n",
      "compound: 0.4404, neg: 0.0, neu: 0.58, pos: 0.42, \n",
      "This movie was actually neither that funny, nor super witty.\n",
      "compound: -0.6759, neg: 0.41, neu: 0.59, pos: 0.0, \n",
      "This movie doesn't care about cleverness, wit or any other kind of    intelligent humor.\n",
      "compound: -0.1338, neg: 0.265, neu: 0.497, pos: 0.239, \n",
      "Those who find ugly meanings in beautiful things are corrupt without    being charming.\n",
      "compound: -0.3553, neg: 0.314, neu: 0.493, pos: 0.192, \n",
      "There are slow and repetitive parts, BUT it has just enough spice to    keep it interesting.\n",
      "compound: 0.4678, neg: 0.079, neu: 0.735, pos: 0.186, \n",
      "The script is not fantastic, but the acting is decent and the cinematography    is EXCELLENT!\n",
      "compound: 0.7565, neg: 0.092, neu: 0.607, pos: 0.301, \n",
      "Roger Dodger is one of the most compelling variations on this theme.\n",
      "compound: 0.2944, neg: 0.0, neu: 0.834, pos: 0.166, \n",
      "Roger Dodger is one of the least compelling variations on this theme.\n",
      "compound: -0.1695, neg: 0.132, neu: 0.868, pos: 0.0, \n",
      "Roger Dodger is at least compelling as a variation on the theme.\n",
      "compound: 0.2263, neg: 0.0, neu: 0.84, pos: 0.16, \n",
      "they fall in love with the product\n",
      "compound: 0.6369, neg: 0.0, neu: 0.588, pos: 0.412, \n",
      "but then it breaks\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "usually around the time the 90 day warranty expires\n",
      "compound: 0.0, neg: 0.0, neu: 1.0, pos: 0.0, \n",
      "the twin towers collapsed today\n",
      "compound: -0.2732, neg: 0.344, neu: 0.656, pos: 0.0, \n",
      "However, Mr. Carter solemnly argues, his client carried out the kidnapping    under orders and in the ''least offensive way possible.''\n",
      "compound: -0.5859, neg: 0.23, neu: 0.697, pos: 0.074, \n"
     ]
    }
   ],
   "source": [
    "sentences.extend(tricky_sentences)\n",
    "for sentence in sentences:\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    print(sentence)\n",
    "    ss = sid.polarity_scores(sentence)\n",
    "    for k in sorted(ss):\n",
    "        print('{0}: {1}, '.format(k, ss[k]), end='')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilidad con NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga de librerias y paquetes necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.test.probability_fixt import setup_module\n",
    "setup_module()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ejemplo de 2 textos similares:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = ['no', 'good', 'fish', 'goes', 'anywhere', 'without', 'a', 'porpoise', '!']\n",
    "text2 = ['no', 'good', 'porpoise', 'likes', 'to', 'fish', 'fish', 'anywhere', '.']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fish', 3),\n",
       " ('anywhere', 2),\n",
       " ('good', 2),\n",
       " ('no', 2),\n",
       " ('porpoise', 2),\n",
       " ('!', 1),\n",
       " ('.', 1),\n",
       " ('a', 1),\n",
       " ('goes', 1),\n",
       " ('likes', 1),\n",
       " ('to', 1),\n",
       " ('without', 1)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "both = nltk.FreqDist(text1 + text2)\n",
    "both_most_common = both.most_common()\n",
    "list(itertools.chain(*(sorted(ys) for k, ys in itertools.groupby(both_most_common, key=lambda t: t[1]))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 2, 'a': 1})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist('abbbc') - FreqDist('bccd')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 4, 'c': 2, 'a': 1})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist('abbb') + FreqDist('bcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 3, 'c': 2, 'a': 1})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist('abbb') | FreqDist('bcc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FreqDist('abbb') & FreqDist('bcc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Descarga de texto de la libreria nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\leopa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\brown.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "corpus = nltk.corpus.brown.tagged_sents(categories='adventure')[:500]\n",
    "print(len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('Dan', 'NP'), ('Morgan', 'NP'), ('told', 'VBD'), ('himself', 'PPL'), ('he', 'PPS'), ('would', 'MD'), ('forget', 'VB'), ('Ann', 'NP'), ('Turner', 'NP'), ('.', '.')], [('He', 'PPS'), ('was', 'BEDZ'), ('well', 'RB'), ('rid', 'JJ'), ('of', 'IN'), ('her', 'PPO'), ('.', '.')], ...]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se separa el texto en etiquetas y simbolos. Se crea un subconjunto de entrenamiento, para todo el texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92\n",
      "1464\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import unique_list\n",
    "tag_set = unique_list(tag for sent in corpus for (word,tag) in sent)\n",
    "print(len(tag_set))\n",
    "symbols = unique_list(word for sent in corpus for (word,tag) in sent)\n",
    "print(len(symbols))\n",
    "trainer = nltk.tag.HiddenMarkovModelTrainer(tag_set, symbols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se divide el conjunto de datos en 90% y 10% para entrenamiento y testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "450\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "train_corpus = []\n",
    "test_corpus = []\n",
    "for i in range(len(corpus)):\n",
    "    if i % 10:\n",
    "        train_corpus += [corpus[i]]\n",
    "    else:\n",
    "        test_corpus += [corpus[i]]\n",
    "print(len(train_corpus))\n",
    "print(len(test_corpus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilización del modelo HMM (Hiden Markov Model), en una función"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(est):\n",
    "    hmm = trainer.train_supervised(train_corpus, estimator=est)\n",
    "    print('%.2f%%' % (100 * hmm.accuracy(test_corpus)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Máxima verosimilitud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.75%\n"
     ]
    }
   ],
   "source": [
    "mle = lambda fd, bins: MLEProbDist(fd)\n",
    "train_and_test(mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Laplace (= Lidstone con gamma==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66.04%\n"
     ]
    }
   ],
   "source": [
    "train_and_test(LaplaceProbDist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimacion de verosimilitud (= Lidstone con gamma==0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.01%\n"
     ]
    }
   ],
   "source": [
    "train_and_test(ELEProbDist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82.51%\n",
      "73.01%\n",
      "66.04%\n"
     ]
    }
   ],
   "source": [
    "def lidstone(gamma):\n",
    "    return lambda fd, bins: LidstoneProbDist(fd, gamma, bins)\n",
    "train_and_test(lidstone(0.1))\n",
    "train_and_test(lidstone(0.5))\n",
    "train_and_test(lidstone(1.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
